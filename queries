# Main SQL queries I use in my day to day. I tend to combine a lot of these together depending on what needs to be done.

# Apache Spark

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SQLinPySpark").getOrCreate()

# Subqueries
spark.sql("""
SELECT 
  YEAR(c.date) AS YearDate,
  COUNT(DISTINCT(c.Id)) AS CanadianCustomers,
  tc.AllCustomers AS AllCustomers,
  (COUNT(DISTINCT(c.Id)) / tc.AllCustomers) AS CanadianCustomerPercentage
FROM customers c
INNER JOIN ( -- Subquery listed here
  SELECT 
    YEAR(to.date) AS TotalYearDate,
    COUNT(DISTINCT(to.Id)) AS AllCustomers
  FROM customers c
) tc 
  ON totalcustomers.totalYearDate = YEAR(to.date)
WHERE c.country = 'Canada'
GROUP BY ALL
;""").show()

# Having with more than 4 sales
spark.sql("""
SELECT 
  c.Id AS CustomerId,
  COUNT(DISTINCT(t.Id)) AS NumberofTransactions
FROM customers c
INNER JOIN transactions t
  ON t.customerId = c.Id
GROUP BY ALL
HAVING COUNT(DISTINCT(t.Id)) >= 4
ORDER BY COUNT(DISTINCT(t.Id)) DESC
;""").show()

# CTE
spark.sql("""
WITH CustomerTransactions AS (
SELECT 
  c.Id AS CustomerId,
  COUNT(DISTINCT(t.Id)) AS NumberofTransactions
FROM customers c
INNER JOIN transactions t
  ON t.customerId = c.Id
GROUP BY ALL
)

SELECT
  SUM(ct.NumberofTransactions) AS TotalTransactions, -- # Number of total customer transactions
  AVG(ct.NumberofTransactions) AS AverageNumberofTransactions -- # of Transactions
FROM CustomerTransactions ct
;""").show()

# Partion 
spark.sql("""
WITH RecentTransaction AS (
SELECT 
t.CustomerId AS CustomerId,
t.AmountPaid AS LastAmountPaid,
ROW_NUMBER() OVER (PARTITION BY t.CustomerId ORDER BY t.PurchaseDate DESC) AS MostRecentDate
FROM transactions t
)

SELECT 
*
customers c
INNER JOIN RecentTransaction rt
  ON rt.CustomerId = c.Id AND rt.MostRecentDate = 1 -- Getting the most recent date 
;""").show()

# Creating tables / views

spark.sql("""
CREATE OR REPLACE TABLE toptencustomers ( -- Note: Can change table to VIEW in this case.
SELECT 
  c.Id AS CustomerId,
  COUNT(DISTINCT(t.Id)) AS NumberofTransactions
FROM customers c
INNER JOIN transactions t
  ON t.customerId = c.Id
GROUP BY ALL
ORDER BY COUNT(DISTINCT(t.Id)) DESC
LIMIT 10
)
;""").show()

# Case statement - Using the CTE in the previous statement for simplicity sake
SELECT 
CASE
  WHEN DATEDIFF(DAY, rt.MostRecentDate, GETDATE()) <= 7 THEN "Transactionwithin7Days"
  WHEN DATEDIFF(DAY, rt.MostRecentDate, GETDATE()) BETWEEN 8 AND 14 THEN "TransactionBetweenEightAndFourteenDays"
  ELSE "TransactionsOverFourteenDays"
END AS 'DaysFromLastTransaction',
COUNT(DISTINCT(c.Id)) AS NumberOfCustomers
customers c
INNER JOIN RecentTransaction rt
  ON rt.CustomerId = c.Id AND rt.MostRecentDate = 1 -- Getting the most recent date
WHERE DATEDIFF(DAY, rt.MostRecentDate, GETDATE()) <= 60 -- Only looking at the past 60 days worth of data.
GROUP BY ALL
;""").show()

# Aggregate functions
spark.sql("""
SELECT 
  c.Id AS CustomerId,
  t.TransactionDate AS TransactionDate,
  SUM(t.AmountPaid) OVER (PARTITION BY c.CustomerId ORDER BY t.TransactionDate ASC) AS CumulativeAmount
FROM customers c
INNER JOIN transactions t
  ON t.CustomerId = c.Id
GROUP BY c.Id, t.TransactionDate
;""").show()
